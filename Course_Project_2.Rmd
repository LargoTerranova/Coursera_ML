---
title: "Coursera Machine Learning Course Project"
author: "Boban D"
date: "3 Juli 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
setwd("D:/Programme/Drive/DataScience/8_ML/Project/")

library(tidyverse)
library(caret)
library(knitr)
library(rattle)
library(randomForest)
```




#######################################
# Loading Data
#######################################
We load the data directly from the url and partition the trainingset into a training- and test set.

```{r}
url_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url_test  <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training        <- read.csv(url(url_train))
testing_final   <- read.csv(url(url_test))

training_partition  <- createDataPartition(training$classe, p=0.7, list=FALSE)
training_set        <- training[training_partition, ]
testing_set         <- training[-training_partition, ]
```


#######################################
# Cleaning Data
#######################################
In this step we identify columns with very low variance and remove them. A variable with a low variance (in the extremest case a constant) does not vary much or at all and might contribute very little to an analysis. We identify 55 columns with low variance and remove them.
```{r}
no_variance_list <- nearZeroVar(training_set)
training_set     <- training_set[, -no_variance_list]
testing_set      <- testing_set[ , -no_variance_list]
testing_final    <- testing_final[, -no_variance_list] 
```


# Calculate missing values per column using purr
Many Machine Learning Algorithms cannot cope with missing data. They either reach wrong results or the clculation is stoped altogether. In this step we identify 59 columns which have missing values in them and remove them.

```{r}
training_set %>%
    map_df(function(x) sum(is.na(x))) %>%
    gather(feature, num_nulls) %>%
    filter(num_nulls==0) -> zero_value_columns

training_set <- training_set[, zero_value_columns$feature]
testing_set  <- testing_set[ , zero_value_columns$feature]

training_set <- training_set[, -(1:5)]
testing_set  <- testing_set[ , -(1:5)] 

testing_final %>%
    map_df(function(x) sum(is.na(x))) %>%
    gather(feature, num_nulls) %>%
    filter(num_nulls==0) -> zero_value_columns_final

testing_final <- testing_final[, zero_value_columns_final$feature]
testing_final <- testing_final[, -(1:5)]
```



#######################################
# Model Fitting
#######################################
## Random Forest
We first fit a Random Forest. We instantly achieve a 99.9% Accuracy with our testset as can be seen in the ConfusionMAtrix. Plotting the results confirms the results.

```{r, echo=FALSE, cache=TRUE}
set.seed(246810)
control_model <- trainControl(method = "cv", number = 2, verboseIter = F)
model_rf <- train(classe~., method="rf", data=training_set, trControl=control_model)
model_rf$finalModel

#Prediction
model_rf_predict    <- predict(model_rf, newdata = testing_set)
model_rf_predict_cm <- confusionMatrix(model_rf_predict, testing_set$classe)
model_rf_predict_cm

plot(model_rf_predict_cm$table, main= 'Random Forest Prediction: Accuracy = 0.9985')
```




## Boosted Model
In order to validate our results we try a second type of model, Boosting in this case. We achieve a 98.71 % Accuracy which is only a fraction lower than the one we achieved with a random forest. A plot again confirms the results.


```{r, echo=FALSE, cache=TRUE}
set.seed(246810)
control_model <- trainControl(method = 'repeatedcv', number = 4, repeats = 1)
model_bm      <- train(classe~., method="gbm", verbose=F, trControl=control_model, data = training_set)
model_bm$finalModel

#Prediction
model_bm_predict    <- predict(model_bm, newdata = testing_set)
model_bm_predict_cm <- confusionMatrix(model_bm_predict, testing_set$classe) 
model_bm_predict_cm

plot(model_bm_predict_cm$table, main= "Boosted Model: Accuracy = 0.9861")
```



#######################################
# Quiz prediction
#######################################
In our final step we use the previously developed models to predict the 'true' testset provided. Using both methods we reach identidal results.

## Using Random Forest
```{r, echo=FALSE}
Quiz_prediction_rf <- predict(model_bm, newdata = testing_final)
Quiz_prediction_rf
```

## Using a Boosted Model
```{r, echo=FALSE}
Quiz_prediction_bm <- predict(model_rf, newdata = testing_final)
Quiz_prediction_bm
```



